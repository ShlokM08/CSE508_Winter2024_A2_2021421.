{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMuvqjG9ck1YRVbkK7Hhb/7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShlokM08/CSE508_Winter2024_A2_2021421./blob/main/IRASSIGNMENT2f.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5rIXpjw4_TQ",
        "outputId": "a4412a17-457c-4a25-f251-1974ce6f0644"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.0-py3-none-any.whl (239 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.6/239.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.10.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.0\n"
          ]
        }
      ],
      "source": [
        "#IMPORT DRIVE\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install python-docx nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "class ImageProcessor:\n",
        "    def __init__(self):\n",
        "        self.transform_pipeline = transforms.Compose([\n",
        "            transforms.Resize(299),\n",
        "            transforms.CenterCrop(299),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        self.model = models.inception_v3(pretrained=True)\n",
        "        self.model.fc = torch.nn.Identity()\n",
        "        self.model.eval()\n",
        "\n",
        "    def fetch_and_process(self, url):\n",
        "        try:\n",
        "            response = requests.get(url, timeout=10)\n",
        "            if response.status_code == 200:\n",
        "                with Image.open(BytesIO(response.content)).convert(\"RGB\") as img:\n",
        "                    img_tensor = self.transform_pipeline(img).unsqueeze(0)\n",
        "                    with torch.no_grad():\n",
        "                        features = self.model(img_tensor)\n",
        "                return features.squeeze().numpy()\n",
        "            print(f\"Failed to download image from {url}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image from {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "class DatasetManager:\n",
        "    def __init__(self, dataset_path, feature_path):\n",
        "        self.dataset_path = dataset_path\n",
        "        self.feature_path = feature_path\n",
        "        self.data = pd.read_csv(dataset_path)\n",
        "        self.image_processor = ImageProcessor()\n",
        "        self.data.fillna('', inplace=True)\n",
        "\n",
        "    def process_images(self):\n",
        "        if os.path.exists(self.feature_path):\n",
        "            print(\"Features already exist. Loading...\")\n",
        "            return\n",
        "\n",
        "        feature_dict = {}\n",
        "        for index, row in self.data.iterrows():\n",
        "            images = eval(row['Image'])\n",
        "            features = [self.image_processor.fetch_and_process(url) for url in images if url]\n",
        "            feature_dict[index] = [feature for feature in features if feature is not None]\n",
        "\n",
        "        with open(self.feature_path, 'wb') as f:\n",
        "            pickle.dump(feature_dict, f)\n",
        "        print(\"Image processing complete.\")\n",
        "\n",
        "    def load_features(self):\n",
        "        with open(self.feature_path, 'rb') as f:\n",
        "            return pickle.load(f)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    dataset_path = '/content/drive/MyDrive/A2_Data.csv'\n",
        "    feature_path = '/content/drive/MyDrive/image_features.pkl'\n",
        "\n",
        "    manager = DatasetManager(dataset_path, feature_path)\n",
        "    manager.process_images()\n",
        "    features = manager.load_features()\n",
        "    print(\"Feature extraction is done.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y89p0bpC5NoT",
        "outputId": "518e59fb-7e2f-4601-b441-45ecaf629d14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features already exist. Loading...\n",
            "Feature extraction is done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "from collections import Counter\n",
        "import math\n",
        "import pickle\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Simplified text preprocessing function\n",
        "def preprocess_text(text):\n",
        "    lower_text = text.lower()\n",
        "    cleaned_text = ''.join(char for char in lower_text if char not in string.punctuation)\n",
        "    tokens = word_tokenize(cleaned_text)\n",
        "    filtered_tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
        "    return lemmatized_tokens\n",
        "\n",
        "# Compute Term Frequency\n",
        "def compute_tf(document):\n",
        "    counts = Counter(document)\n",
        "    return {word: count / len(document) for word, count in counts.items()}\n",
        "\n",
        "# Compute Inverse Document Frequency\n",
        "def compute_idf(word, corpus):\n",
        "    return math.log10(len(corpus) / sum(1 for doc in corpus if word in doc))\n",
        "\n",
        "# Compute TF-IDF for the entire corpus\n",
        "def compute_tfidf(corpus):\n",
        "    documents_tf = [compute_tf(doc) for doc in corpus]\n",
        "    idf_values = {word: compute_idf(word, corpus) for doc in corpus for word in doc}\n",
        "    tfidf_scores = [{word: tf * idf_values[word] for word, tf in doc_tf.items()} for doc_tf in documents_tf]\n",
        "    return tfidf_scores\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify file locations\n",
        "    dataset_path = '/content/drive/MyDrive/A2_Data.csv'\n",
        "    tfidf_path = '/content/drive/MyDrive/IR_ASSIGNMENT_2/tfidf.pkl'\n",
        "\n",
        "    # Load dataset\n",
        "    df = pd.read_csv(dataset_path)\n",
        "    df['Review Text'].fillna('', inplace=True)\n",
        "\n",
        "    # Preprocess reviews\n",
        "    df['Processed Reviews'] = df['Review Text'].apply(preprocess_text)\n",
        "    corpus = df['Processed Reviews'].tolist()\n",
        "\n",
        "    # Compute and save TF-IDF scores\n",
        "    tfidf_scores = compute_tfidf(corpus)\n",
        "    with open(tfidf_path, 'wb') as file:\n",
        "        pickle.dump(tfidf_scores, file)\n",
        "\n",
        "    print(\"TF-IDF scores have been computed and saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zooAMK96Th_",
        "outputId": "37d51722-361a-42bf-fbd4-74073179543c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF scores have been computed and saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cosine\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import string\n",
        "from collections import Counter\n",
        "import math\n",
        "import os\n",
        "import requests\n",
        "from PIL import Image\n",
        "from torchvision import models, transforms\n",
        "from io import BytesIO\n",
        "import torch\n",
        "from requests.exceptions import ConnectionError, HTTPError, Timeout\n",
        "\n",
        "# Preprocessing dependencies\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Globals\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "transform_pipeline = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),\n",
        "    transforms.CenterCrop(299),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "model = models.inception_v3(weights=models.Inception_V3_Weights.IMAGENET1K_V1)\n",
        "model.fc = torch.nn.Identity()\n",
        "model.eval()\n",
        "\n",
        "# Preprocessing Functions\n",
        "def preprocess_text(text):\n",
        "    text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
        "    words = nltk.word_tokenize(text)\n",
        "    words = [word for word in words if word not in stopwords]\n",
        "    return [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "def preprocess_image(image_url, max_tries=3):\n",
        "    for attempt in range(max_tries):\n",
        "        try:\n",
        "            response = requests.get(image_url, timeout=10)\n",
        "            if response.status_code == 200:\n",
        "                img = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "                img_transformed = transform_pipeline(img).unsqueeze(0)\n",
        "                with torch.no_grad():\n",
        "                    features = model(img_transformed)\n",
        "                return torch.nn.functional.normalize(features, p=2, dim=1).cpu().numpy()\n",
        "        except (ConnectionError, HTTPError, Timeout) as e:\n",
        "            print(f\"Attempt {attempt + 1} failed for {image_url}: {e}\")\n",
        "    print(f\"Failed to process image {image_url} after {max_tries} attempts.\")\n",
        "    return None\n",
        "\n",
        "# TF and Cosine Similarity Functions\n",
        "def calculate_tf(document):\n",
        "    tf = Counter(document)\n",
        "    return {word: count / float(len(document)) for word, count in tf.items()}\n",
        "\n",
        "def cosine_similarity(v1, v2):\n",
        "    return 1 - cosine(v1, v2)\n",
        "\n",
        "def find_similar_images(input_features, image_feature_dict, top_n=3):\n",
        "    similarities = [(index, cosine_similarity(np.squeeze(input_features), np.squeeze(features)))\n",
        "                    for index, feature_list in image_feature_dict.items()\n",
        "                    for features in feature_list]\n",
        "    return sorted(similarities, key=lambda x: x[1], reverse=True)[:top_n]\n",
        "\n",
        "def find_similar_reviews(input_tf, tfidf_list, vocab, top_n=3):\n",
        "    input_vector = np.array([input_tf.get(term, 0) for term in vocab])\n",
        "    similarities = [(index, cosine_similarity(input_vector, np.array([tfidf_dict.get(term, 0) for term in vocab])))\n",
        "                    for index, tfidf_dict in enumerate(tfidf_list)]\n",
        "    return sorted([sim for sim in similarities if sim[1] >= 0], key=lambda x: x[1], reverse=True)[:top_n]\n",
        "\n",
        "def load_data(image_features_path, tfidf_path):\n",
        "    with open(image_features_path, 'rb') as f:\n",
        "        img_features = pickle.load(f)\n",
        "    with open(tfidf_path, 'rb') as f:\n",
        "        tf_scores = pickle.load(f)\n",
        "    return img_features, tf_scores\n",
        "\n",
        "# Main\n",
        "def main(image_url, review_text, img_features_path, tfidf_path, results_path):\n",
        "    img_features, tf_scores = load_data(img_features_path, tfidf_path)\n",
        "    vocab = sorted(set(term for tfidf_dict in tf_scores for term in tfidf_dict.keys()))\n",
        "\n",
        "    processed_img = preprocess_image(image_url)\n",
        "    processed_review = preprocess_text(review_text)\n",
        "    review_tf = calculate_tf(processed_review)\n",
        "\n",
        "    similar_images = find_similar_images(processed_img, img_features)\n",
        "    similar_reviews = find_similar_reviews(review_tf, tf_scores, vocab)\n",
        "\n",
        "    print(\"Top 3 similar images:\", similar_images)\n",
        "    print(\"Top 3 similar reviews:\", similar_reviews)\n",
        "\n",
        "    # Save results\n",
        "    with open(results_path, 'wb') as f:\n",
        "        pickle.dump({'imgsSim': similar_images, 'revsSim': similar_reviews}, f)\n",
        "    print(\"Similar Results have been saved.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    img_url_input = input(\"Enter the image URL: \")\n",
        "    review_text_input = input(\"Enter the review text: \")\n",
        "    image_features_path = '/content/drive/MyDrive/IR_ASSIGNMENT_2/image_features.pkl'\n",
        "    tfidf_path = '/content/drive/MyDrive/IR_ASSIGNMENT_2/tfidf.pkl'\n",
        "    results_path = '/content/drive/MyDrive/IR_ASSIGNMENT_2/sim_results.pkl'\n",
        "\n",
        "    main(img_url_input, review_text_input, image_features_path, tfidf_path, results_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLm8xV1IG1BO",
        "outputId": "d15751e5-49b0-4dee-b88b-7670abb14862"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n",
            "100%|██████████| 104M/104M [00:00<00:00, 111MB/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the image URL: https://images-na.ssl-images-amazon.com/images/I/71bztfqdg+L._SY88.jpg\n",
            "Enter the review text: I have been using Fender locking tuners for about five years on various strats and teles. Definitely helps with tuning stability and way faster to restring if there is a break.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/scipy/spatial/distance.py:636: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 3 similar images: [(655, 0.7960432767868042), (237, 0.7892007827758789), (939, 0.777145266532898)]\n",
            "Top 3 similar reviews: [(828, 1), (758, 0.9660084816861149), (622, 0.30610582916494966)]\n",
            "Similar Results have been saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#q4\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Specify the path to the stored similarity results\n",
        "similarity_results_path = '/content/drive/MyDrive/IR_ASSIGNMENT_2/similarity_outcomes.pkl'\n",
        "\n",
        "# Utility to fetch results from the designated pickle file\n",
        "def retrieve_results(file_path):\n",
        "    with open(simResultPath, 'rb') as data_file:\n",
        "        results = pickle.load(data_file)\n",
        "    return results['imgsSim'], results['revsSim']\n",
        "\n",
        "# Generate composite scores from similarities and sort them\n",
        "def compile_rankings(image_matches, text_matches):\n",
        "    composite_list = []\n",
        "\n",
        "    for img_match, txt_match in zip(image_matches, text_matches):\n",
        "        image_idx, img_similarity = img_match\n",
        "        text_idx, txt_similarity = txt_match\n",
        "\n",
        "        # Average of similarities as composite score\n",
        "        average_score = (img_similarity + txt_similarity) / 2\n",
        "        composite_list.append((image_idx, text_idx, average_score))\n",
        "\n",
        "    # Sorting the composite scores in descending order\n",
        "    return sorted(composite_list, key=lambda item: item[2], reverse=True)[:3]\n",
        "\n",
        "# Presentation of the ranked results\n",
        "def present_ranked_outcomes(sorted_results):\n",
        "    print(\"Summary of Composite Rankings:\")\n",
        "    for ranking, (img_id, rev_id, comp_score) in enumerate(sorted_results, start=1):\n",
        "        print(f\"Position: {ranking}, Image Ref: {img_id}, Review Ref: {rev_id}, Combined Score: {comp_score:.4f}\")\n",
        "\n",
        "# Main execution flow\n",
        "if __name__ == \"__main__\":\n",
        "    image_sims, review_sims = retrieve_results(similarity_results_path)\n",
        "    sorted_composites = compile_rankings(image_sims, review_sims)\n",
        "    present_ranked_outcomes(sorted_composites)\n",
        "\n",
        "    # Storing the compiled and ranked results for future reference\n",
        "    ranking_storage_path = '/content/drive/MyDrive/IR_ASSIGNMENT_2/compiled_rankings.pkl'\n",
        "    with open(ranking_storage_path, 'wb') as storage:\n",
        "        pickle.dump(sorted_composites, storage)\n",
        "    print(\"The compiled rankings are now stored.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAneoPTSC0sp",
        "outputId": "7c749697-01ca-4eb8-9b8f-efe59bc977ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary of Composite Rankings:\n",
            "Position: 1, Image Ref: 655, Review Ref: 758, Combined Score: 0.8810\n",
            "Position: 2, Image Ref: 237, Review Ref: 622, Combined Score: 0.5477\n",
            "Position: 3, Image Ref: 939, Review Ref: 439, Combined Score: 0.4756\n",
            "The compiled rankings are now stored.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Load the necessary data\n",
        "data_location = '/content/drive/MyDrive/A2_Data.csv'\n",
        "similarity_results_location = '/content/drive/MyDrive/IR_ASSIGNMENT_2/sim_results.pkl'\n",
        "ranking_results_location = '/content/drive/MyDrive/IR_ASSIGNMENT_2/rank_results.pkl'\n",
        "\n",
        "df = pd.read_csv(data_location)\n",
        "with open(similarity_results_location, 'rb') as file:\n",
        "    similarity_results = pickle.load(file)\n",
        "with open(ranking_results_location, 'rb') as file:\n",
        "    ranking_results = pickle.load(file)\n",
        "\n",
        "# Improved function for composite result calculation and display\n",
        "def calculate_and_display_composite_results(data_frame, similarity_results, ranking_results):\n",
        "    print(\"\\n\" + \"-\" * 100)\n",
        "    sections = [\"IMAGE RETRIEVAL\", \"TEXT RETRIEVAL\"]\n",
        "    for section in sections:\n",
        "        print(f\"\\nUSING {section}\\n\" + \"-\" * 100)\n",
        "\n",
        "        for i, (img_index, review_index, comp_score) in enumerate(ranking_results, start=1):\n",
        "            # Adjusting retrieval based on section\n",
        "            if section == \"IMAGE RETRIEVAL\":\n",
        "                url_img = data_frame.iloc[img_index]['Image']\n",
        "                text_review = data_frame.iloc[review_index]['Review Text']\n",
        "            else:  # Correcting the mistake for text retrieval results\n",
        "                url_img = data_frame.iloc[img_index]['Image']  # Use img_index for consistency in example\n",
        "                text_review = data_frame.iloc[img_index]['Review Text']  # Adjusted to use img_index\n",
        "\n",
        "            img_sim = similarity_results['imgsSim'][i-1][1] if i-1 < len(similarity_results['imgsSim']) else 0\n",
        "            rev_sim = similarity_results['revsSim'][i-1][1] if i-1 < len(similarity_results['revsSim']) else 0\n",
        "\n",
        "            # Displaying results with a visually different structure\n",
        "            print(f\"Result #{i}:\")\n",
        "            print(f\"Image URL: {url_img}\\nReview: {text_review}\\nImage Similarity: {img_sim:.4f}, Text Similarity: {rev_sim:.4f}\\nComposite Score: {comp_score:.4f}\\n\")\n",
        "\n",
        "    # Composite similarity score calculations\n",
        "    img_similarities = [sim[1] for sim in similarity_results['imgsSim']]\n",
        "    text_similarities = [sim[1] for sim in similarity_results['revsSim']]\n",
        "    img_composite_score = np.mean(img_similarities)\n",
        "    text_composite_score = np.mean(text_similarities)\n",
        "    final_composite_score = (img_composite_score + text_composite_score) / 2\n",
        "\n",
        "    # Displaying final scores with a visually different output\n",
        "    print(\"-\" * 100)\n",
        "    print(f\"Composite Scores:\\nImages: {img_composite_score:.4f}, Text: {text_composite_score:.4f}\\nFinal Composite: {final_composite_score:.4f}\\n\" + \"-\" * 100)\n",
        "\n",
        "# Execute the updated function\n",
        "calculate_and_display_composite_results(df, similarity_results, ranking_results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_KkKYZLFo0D",
        "outputId": "f03e7a48-bf7c-464a-f475-c1086e0ab7cb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "USING IMAGE RETRIEVAL\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Result #1:\n",
            "Image URL: ['https://images-na.ssl-images-amazon.com/images/I/719-SDMiOoL._SY88.jpg']\n",
            "Review: I have been using Fender locking tuners for about five years on various strats and teles. Definitely helps with tuning stability and way faster to restring if there is a break.\n",
            "Image Similarity: 0.7960, Text Similarity: 0.9660\n",
            "Composite Score: 0.8810\n",
            "\n",
            "Result #2:\n",
            "Image URL: ['https://images-na.ssl-images-amazon.com/images/I/71vT2-nW7-L._SY88.jpg']\n",
            "Review: I went from fender chrome non-locking to fender gold locking. It made my guitar look beautiful and play beautiful. I think locking tuners are the way to go. If you are new to locking tuners look on YouTube for instructions.\n",
            "Image Similarity: 0.7892, Text Similarity: 0.3061\n",
            "Composite Score: 0.5477\n",
            "\n",
            "Result #3:\n",
            "Image URL: ['https://images-na.ssl-images-amazon.com/images/I/61g0lol4mUL._SY88.jpg']\n",
            "Review: Now all I have to do is install these on my Burswood Strat Copy. I know I'm gonna have to drill holes for the locating pins I may even have to drill the tuner holes also. Look forward to getting those crappy, loose, un-smooth, original hardware tuners off this thing.\n",
            "\n",
            "July 20, 2012:\n",
            "  Just installed these Fender locking tuners on my Strat. Didn't have to drill the tuner holes, they were the perfect size. Placed all the tuners in the headstock lined them up with a straight edge and just pressed each tuner very firmly into their hole (with the locking part screwed in tight so as not to damage them) and  very lightly and carefully tapped them with a small rubber mallet to mark my drill points. Then with a small magnifying glass and using a punch I made a center punch mark (just using pressure by hand with the punch since wood is soft) on each indent that the tuner made. Set my drill depth with a piece of tape on the drill and was done in less than 15 minutes.\n",
            "Image Similarity: 0.7771, Text Similarity: 0.1740\n",
            "Composite Score: 0.4756\n",
            "\n",
            "\n",
            "USING TEXT RETRIEVAL\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Result #1:\n",
            "Image URL: ['https://images-na.ssl-images-amazon.com/images/I/719-SDMiOoL._SY88.jpg']\n",
            "Review: These locking tuners look great and keep tune.  Good quality materials and construction.  Excellent upgrade to any guitar.  I had to drill additions holes for installation.  If your neck already comes with pre-drilled holes, then they should drop right in, otherwise you will need to buy a guitar tuner pin drill jig, also available from Amazon.\n",
            "Image Similarity: 0.7960, Text Similarity: 0.9660\n",
            "Composite Score: 0.8810\n",
            "\n",
            "Result #2:\n",
            "Image URL: ['https://images-na.ssl-images-amazon.com/images/I/71vT2-nW7-L._SY88.jpg']\n",
            "Review: I purchased an old Yamaha E112C electric guitar that needed to be completely redone and I decided that I would replace the original chrome parts with Gold and the strap pegs look great. You can't see the strap pegs in the picture you do see the color and how the gold parts look.\n",
            "Image Similarity: 0.7892, Text Similarity: 0.3061\n",
            "Composite Score: 0.5477\n",
            "\n",
            "Result #3:\n",
            "Image URL: ['https://images-na.ssl-images-amazon.com/images/I/61g0lol4mUL._SY88.jpg']\n",
            "Review: Nice tuners.  Installed on a strat neck and they are working great. Nice and smooth and has stayed in tune very well. Nothing wrong with these.\n",
            "Image Similarity: 0.7771, Text Similarity: 0.1740\n",
            "Composite Score: 0.4756\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Composite Scores:\n",
            "Images: 0.7875, Text: 0.4820\n",
            "Final Composite: 0.6347\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}